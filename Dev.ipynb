{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from github_python_fetch import fetch_function\n",
    "from query_package import get_query_package\n",
    "\n",
    "# GitHub Access \n",
    "token = os.getenv('GITHUBTOKEN')\n",
    "\n",
    "# AWS Access\n",
    "access = os.getenv('ACCESS')\n",
    "secret = os.getenv('SECRET')\n",
    "\n",
    "# RDS Access\n",
    "username = os.getenv('USER')\n",
    "password = os.getenv('PASSWORD')\n",
    "server = os.getenv('SERVER')\n",
    "db = os.getenv('DB')\n",
    "\n",
    "\n",
    "# Access RDS Functions via GitHub\n",
    "rds_functions_url = 'https://raw.githubusercontent.com/CharlesRoss-MBakerIntl/Tidal-Basin-Functions/main/rds_connector.py' # Set url to python file of github\n",
    "rds_functions = fetch_function(rds_functions_url, token) # Pull function from github using requests\n",
    "exec(rds_functions) # Execute the file\n",
    "\n",
    "\n",
    "# Pull Query Package from File\n",
    "query_package = get_query_package()\n",
    "\n",
    "\n",
    "# Store RDS Functions Locally\n",
    "#conn, cursor = rds_connection(username, password, db, server) # Connect to RDS Database\n",
    "#rds = RDSTablePull(conn, cursor, query_package) # Create Instance of RDS Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in rds.cleaning_versions:\n",
    "    \n",
    "    # Create File Name\n",
    "    if item['Field'] == None:\n",
    "        file_name = f\"{item['Step']}\"\n",
    "    else:\n",
    "        file_name = f\"{item['Field']}: {item['Step']}\"\n",
    "\n",
    "    # Store DataFrame as CSV\n",
    "    #data = item['Result'].to_csv(f\"{file_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket and file information\n",
    "bucket_name = 'reporting-external'\n",
    "\n",
    "# Create a S3 client\n",
    "s3 = boto3.client('s3', aws_access_key_id=access, aws_secret_access_key=secret)\n",
    "\n",
    "result = s3.list_objects_v2(Bucket=bucket_name, Delimiter='/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_active_data(s3, bucket, folder, file_name, data):\n",
    "\n",
    "    #Set Folder Prefix to Active Data Folder\n",
    "    folder_prefix = folder + 'Active-Data/'\n",
    "    \n",
    "    # Check if the file exists in the specified folder\n",
    "    try:\n",
    "        #Pull Results for File in S3 Bucket\n",
    "        result = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_prefix + file_name)\n",
    "\n",
    "        #Check if File Exists in Folder\n",
    "        file_exists = 'Contents' in result and any(item['Key'] == folder_prefix + file_name for item in result['Contents'])\n",
    "\n",
    "        #If the file exists\n",
    "        if file_exists:\n",
    "            try:\n",
    "                \n",
    "                #Grab Final Dataset from Data\n",
    "                final_data = data\n",
    "\n",
    "                # Update the file with a new version\n",
    "                s3.put_object(Bucket = bucket, Key = folder_prefix + file_name, Body = final_data.to_csv())\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error: Failed to Overwrite Active csv file {file_name} in {folder}\")\n",
    "\n",
    "        else:\n",
    "            raise Exception(f\"Error: Could not update active csv, '{file_name}' does not exist within {folder}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error: Could not update active csv file in {folder}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'BWJME6PF5PX44ECH',\n",
       "  'HostId': 'Ol9yXqHDGxAynGLMwoKsCLjfRGTIAWubccH2+SRRabJjv3H/rzNWEzN+dRM8RIwB6AAYfLqPA/VWz3grbs1WQw==',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'Ol9yXqHDGxAynGLMwoKsCLjfRGTIAWubccH2+SRRabJjv3H/rzNWEzN+dRM8RIwB6AAYfLqPA/VWz3grbs1WQw==',\n",
       "   'x-amz-request-id': 'BWJME6PF5PX44ECH',\n",
       "   'date': 'Wed, 30 Oct 2024 17:58:29 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"0ae9bcd0c0b0aa5aab99d84beca26ce8\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"0ae9bcd0c0b0aa5aab99d84beca26ce8\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the CSV to S3, overwriting the existing file\n",
    "s3.put_object(Bucket=bucket_name, Key=\"FDEM-Hotel-Summary/Active-Data/Active_FDEM_Hotel_Summary.csv\", Body=pd.DataFrame().to_csv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_package(data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_active_data(s3, bucket, folder_prefix, data):\n",
    "\n",
    "    active_prefix = folder_prefix + \"Active-Data\"\n",
    "\n",
    "    # List objects within the specified folder\n",
    "    result = s3.list_objects_v2(Bucket=bucket_name, Prefix=active_prefix, Delimiter='/')\n",
    "\n",
    "    #Grab Folder Objects\n",
    "    active_objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_s3_folder(s3, bucket, folder_name, data = None, folder_prefix = None, limit = None):\n",
    "    \n",
    "    #If Folder Path Passed\n",
    "    if folder_prefix is not None:\n",
    "\n",
    "        try:\n",
    "\n",
    "            # List objects within the specified folder\n",
    "            result = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_prefix, Delimiter='/')\n",
    "            \n",
    "            #If Limit is Passed\n",
    "            if limit is not None:\n",
    "                \n",
    "                #Check if CommonPrefixes in Result\n",
    "                if 'CommonPrefixes' in result:\n",
    "                    subfolders = result['CommonPrefixes']   #Find All Sub-Folders within Passed Folder\n",
    "                    folder_count = len(subfolders)   #Store Folder Count\n",
    "\n",
    "                    #Check if Folder Count Over Passed Limit\n",
    "                    if folder_count > limit:\n",
    "                        \n",
    "                        # Fetch all subfolder objects to get their last modified date\n",
    "                        folder_dates = []\n",
    "                        for folder in subfolders:\n",
    "                            \n",
    "                            #Set Folder Key\n",
    "                            folder_key = folder['Prefix']\n",
    "\n",
    "                            #Grab Folder Objects\n",
    "                            folder_objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_key)\n",
    "                            \n",
    "                            #Check if Contents in Folder Objects\n",
    "                            if 'Contents' in folder_objects:\n",
    "                                \n",
    "                                # Get the oldest object's last modified date\n",
    "                                oldest_object = min(folder_objects['Contents'], key=lambda x: x['LastModified'])\n",
    "                                folder_dates.append((folder_key, oldest_object['LastModified']))\n",
    "                            \n",
    "                            #Raise Exception if no Contents Found\n",
    "                            else:\n",
    "                                raise Exception(f\"Contents not found in Folder List in {folder_key}\")\n",
    "\n",
    "\n",
    "                            # Sort folders by date and delete the oldest one\n",
    "                            folder_dates.sort(key=lambda x: x[1])\n",
    "\n",
    "                            # Set Key to Oldest Folder\n",
    "                            oldest_folder_key = folder_dates[0][0]\n",
    "\n",
    "                            #Delete Oldest Folder\n",
    "                            s3.delete_object(Bucket=bucket_name, Key=oldest_folder_key)\n",
    "\n",
    "                            #Add New Folder\n",
    "                            s3.put_object(Bucket=bucket_name, Key=folder_prefix + folder_name)\n",
    "\n",
    "                            #Add Data\n",
    "                            add_data_package(data)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    raise Exception(f\"Error: No folders found in {folder_prefix}\")\n",
    "            \n",
    "\n",
    "            #No Limit, Add Folder to Folder Path\n",
    "            elif limit is None:\n",
    "                \n",
    "                #Add New Folder\n",
    "                s3.put_object(Bucket=bucket_name, Key=folder_prefix + folder_name)\n",
    "\n",
    "                #Add Data\n",
    "                add_data_package(data)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    #No Folder Path Add, Add Folder to Main Bucket\n",
    "    elif folder_prefix is None:\n",
    "        \n",
    "        #Add New Folder\n",
    "        s3.put_object(Bucket=bucket_name, Key=folder_name)\n",
    "\n",
    "        #Add Data\n",
    "        add_data_package(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folders: 1\n",
      "FDEM-Hotel-Summary/Archived-Processing-Steps/2024_10_29/\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "# Bucket and folder information\n",
    "bucket_name = 'reporting-external'\n",
    "folder_prefix = 'FDEM-Hotel-Summary/Archived-Processing-Steps/'\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # List objects within the specified folder\n",
    "    result = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_prefix, Delimiter='/')\n",
    "    \n",
    "    if 'CommonPrefixes' in result:\n",
    "\n",
    "        #Find All Sub-Folders within Passed Folder\n",
    "        subfolders = result['CommonPrefixes']\n",
    "\n",
    "        #Store Folder Count\n",
    "        folder_count = len(subfolders)\n",
    " \n",
    "\n",
    "        #Check if Folder Count Over Passed Limit\n",
    "        if folder_count > 30:\n",
    "            \n",
    "            # Fetch all subfolder objects to get their last modified date\n",
    "            folder_dates = []\n",
    "            for folder in subfolders:\n",
    "                \n",
    "                folder_key = folder['Prefix']\n",
    "                folder_objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_key)\n",
    "                \n",
    "                if 'Contents' in folder_objects:\n",
    "                    # Get the oldest object's last modified date\n",
    "                    oldest_object = min(folder_objects['Contents'], key=lambda x: x['LastModified'])\n",
    "                    folder_dates.append((folder_key, oldest_object['LastModified']))\n",
    "\n",
    "            # Sort folders by date and delete the oldest one\n",
    "            folder_dates.sort(key=lambda x: x[1])\n",
    "            oldest_folder_key = folder_dates[0][0]\n",
    "\n",
    "            print(f\"Deleting the oldest folder: {oldest_folder_key}\")\n",
    "            s3.delete_object(Bucket=bucket_name, Key=oldest_folder_key)\n",
    "            print(f\"Folder '{oldest_folder_key}' has been deleted.\")\n",
    "\n",
    "        else:\n",
    "            for folder in subfolders:\n",
    "                print(folder['Prefix'])\n",
    "    else:\n",
    "        print(f\"No folders found in {folder_prefix}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Prefix': 'FDEM-Hotel-Summary/Archived-Processing-Steps/2024_10_29/'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'GFEY835AM7VDB6DE',\n",
       "  'HostId': 'SicqyPB8hYylUqIYvwUfUwYhVdhM2tjJhRRwIovwBFOeJOWhWiv5PgoPGN+dQz8KRo/aADt0XpY=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'SicqyPB8hYylUqIYvwUfUwYhVdhM2tjJhRRwIovwBFOeJOWhWiv5PgoPGN+dQz8KRo/aADt0XpY=',\n",
       "   'x-amz-request-id': 'GFEY835AM7VDB6DE',\n",
       "   'date': 'Tue, 29 Oct 2024 21:31:50 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.put_object(Bucket=bucket_name, Key='TESTFOLDER')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '65MJD1CJHWFTA60M',\n",
       "  'HostId': '1Lk2nBZUnsjI6amk43T8OpQhnd5geJ/PlUGqMhRLR0hcil7efHpAymVKAuJKR8ReTm6M75IbwCzRCrf3AdU/1g==',\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '1Lk2nBZUnsjI6amk43T8OpQhnd5geJ/PlUGqMhRLR0hcil7efHpAymVKAuJKR8ReTm6M75IbwCzRCrf3AdU/1g==',\n",
       "   'x-amz-request-id': '65MJD1CJHWFTA60M',\n",
       "   'date': 'Tue, 29 Oct 2024 21:33:49 GMT',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.delete_object(Bucket=bucket_name, Key='TESTFOLDER')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tidal_basin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
